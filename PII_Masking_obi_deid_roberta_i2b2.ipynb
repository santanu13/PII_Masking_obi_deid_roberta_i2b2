{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72440c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdde20a68039434caf95532d053150e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=45, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "transformers_model = 'obi/deid_roberta_i2b2' # e.g. \"obi/deid_roberta_i2b2\"\n",
    "\n",
    "snapshot_download(repo_id=transformers_model)\n",
    "\n",
    "# Instantiate to make sure it's downloaded during installation and not runtime\n",
    "AutoTokenizer.from_pretrained(transformers_model)\n",
    "AutoModelForTokenClassification.from_pretrained(transformers_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307b569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry, EntityRecognizer\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "import copy\n",
    "\n",
    "from presidio_analyzer import EntityRecognizer\n",
    "from typing import Optional, List\n",
    "from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForTokenClassification,\n",
    "        pipeline,\n",
    "        TokenClassificationPipeline,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d8acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry, EntityRecognizer\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "import copy\n",
    "\n",
    "from presidio_analyzer import EntityRecognizer\n",
    "from typing import Optional, List\n",
    "from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForTokenClassification,\n",
    "        pipeline,\n",
    "        TokenClassificationPipeline,\n",
    "    )\n",
    "\n",
    "class TransformersRecognizer(EntityRecognizer):\n",
    "  \n",
    "    def load(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: Optional[str] = None,\n",
    "        pipeline: Optional[TokenClassificationPipeline] = None,\n",
    "        supported_entities: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.name = f\"Transformers model {model_path}\"\n",
    "        self.model_path = model_path\n",
    "        self.pipeline = pipeline\n",
    "        self.is_loaded = False\n",
    "\n",
    "        self.aggregation_mechanism = None\n",
    "        self.ignore_labels = None\n",
    "        self.model_to_presidio_mapping = None\n",
    "        self.entity_mapping = None\n",
    "        self.default_explanation = None\n",
    "        self.text_overlap_length = None\n",
    "        self.chunk_length = None\n",
    "        self.id_entity_name = None\n",
    "        self.id_score_reduction = None\n",
    "        self.supported_language = [\"en\"]\n",
    "        self.supported_entities = [\n",
    "        \"LOCATION\",\n",
    "        \"PERSON\",\n",
    "        \"ORGANIZATION\",\n",
    "        \"AGE\",\n",
    "        \"PHONE_NUMBER\",\n",
    "        \"EMAIL\",\n",
    "        \"DATE_TIME\",\n",
    "        \"ZIP\",\n",
    "        \"PROFESSION\",\n",
    "        \"USERNAME\",\n",
    "        \"ID\"]\n",
    "        self._id = 1\n",
    "        \n",
    "    def load_transformer(self, **kwargs) -> None:\n",
    "\n",
    "        self.entity_mapping = kwargs.get(\"DATASET_TO_PRESIDIO_MAPPING\", {})\n",
    "        self.model_to_presidio_mapping = kwargs.get(\"MODEL_TO_PRESIDIO_MAPPING\", {})\n",
    "        self.ignore_labels = kwargs.get(\"LABELS_TO_IGNORE\", [\"O\"])\n",
    "        self.aggregation_mechanism = kwargs.get(\"SUB_WORD_AGGREGATION\", \"simple\")\n",
    "        self.default_explanation = kwargs.get(\"DEFAULT_EXPLANATION\", None)\n",
    "        self.text_overlap_length = kwargs.get(\"CHUNK_OVERLAP_SIZE\", 40)\n",
    "        self.chunk_length = kwargs.get(\"CHUNK_SIZE\", 600)\n",
    "        self.id_entity_name = kwargs.get(\"ID_ENTITY_NAME\", \"ID\")\n",
    "        self.id_score_reduction = kwargs.get(\"ID_SCORE_REDUCTION\", 0.5)\n",
    "        \n",
    "        if not self.pipeline:\n",
    "            if not self.model_path:\n",
    "                self.model_path = \"obi/deid_roberta_i2b2\"\n",
    "\n",
    "        self._load_pipeline()\n",
    "        return self.pipeline\n",
    "\n",
    "    def _load_pipeline(self) -> None:\n",
    "        \"\"\"Initialize NER transformers_rec pipeline using the model_path provided\"\"\"\n",
    "\n",
    "        print(f\"Initializing NER pipeline using\", self.model_path)\n",
    "        device = -1\n",
    "        self.pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=AutoModelForTokenClassification.from_pretrained(self.model_path),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(self.model_path),\n",
    "            # Will attempt to group sub-entities to word level\n",
    "            aggregation_strategy=self.aggregation_mechanism,\n",
    "            device=device,\n",
    "            framework=\"pt\",\n",
    "            ignore_labels=self.ignore_labels,\n",
    "        )\n",
    "        \n",
    "        self.is_loaded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a310f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NER pipeline using obi/deid_roberta_i2b2\n"
     ]
    }
   ],
   "source": [
    "model_path = \"obi/deid_roberta_i2b2\"\n",
    "transformers_recognizer = TransformersRecognizer(model_path=model_path)\n",
    "model_configuration = { \"PRESIDIO_SUPPORTED_ENTITIES\":[ \"LOCATION\", \"PERSON\", \"ORGANIZATION\", \"AGE\", \"PHONE_NUMBER\", \"EMAIL\", \"DATE_TIME\", \"ZIP\", \"PROFESSION\", \"USERNAME\", \"ID\" ], \"DEFAULT_MODEL_PATH\":\"obi/deid_roberta_i2b2\", \"LABELS_TO_IGNORE\":[\"O\"], \"DEFAULT_EXPLANATION\":\"Identifiedas{}bytheobi/deid_roberta_i2b2NERmodel\", \"SUB_WORD_AGGREGATION\":\"simple\", \"DATASET_TO_PRESIDIO_MAPPING\":{ \"DATE\":\"DATE_TIME\", \"DOCTOR\":\"PERSON\", \"PATIENT\":\"PERSON\", \"HOSPITAL\":\"ORGANIZATION\", \"MEDICALRECORD\":\"O\", \"IDNUM\":\"O\", \"ORGANIZATION\":\"ORGANIZATION\", \"ZIP\":\"O\", \"PHONE\":\"PHONE_NUMBER\", \"USERNAME\":\"\", \"STREET\":\"LOCATION\", \"PROFESSION\":\"PROFESSION\", \"COUNTRY\":\"LOCATION\", \"LOCATION-OTHER\":\"LOCATION\", \"FAX\":\"PHONE_NUMBER\", \"EMAIL\":\"EMAIL\", \"STATE\":\"LOCATION\", \"DEVICE\":\"O\", \"ORG\":\"ORGANIZATION\", \"AGE\":\"AGE\", }, \"MODEL_TO_PRESIDIO_MAPPING\":{ \"PER\":\"PERSON\", \"LOC\":\"LOCATION\", \"ORG\":\"ORGANIZATION\", \"AGE\":\"AGE\", \"ID\":\"ID\", \"EMAIL\":\"EMAIL\", \"PATIENT\":\"PERSON\", \"STAFF\":\"PERSON\", \"HOSP\":\"ORGANIZATION\", \"PATORG\":\"ORGANIZATION\", \"DATE\":\"DATE_TIME\", \"PHONE\":\"PHONE_NUMBER\", }, \"CHUNK_OVERLAP_SIZE\":40, \"CHUNK_SIZE\":600, \"ID_SCORE_MULTIPLIER\":0.4, \"ID_ENTITY_NAME\":\"ID\" }\n",
    "pipeline_ = transformers_recognizer.load_transformer(**model_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "733310ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few example sentences we currently support:\n",
      "\n",
      "Hello, my name is David Johnson and I live in Maine.\n",
      "My credit card number is 4095-2609-9393-4932 and my crypto wallet id is 16Yeky6GMjeNkAiNcBY7ZhrLoMSgg1BoyZ.\n",
      "\n",
      "On September 18 I visited microsoft.com and sent an email to test@presidio.site,  from the IP 192.168.0.1.\n",
      "\n",
      "My passport: 191280342 and my phone number: (212) 555-1234.\n",
      "\n",
      "This is a valid International Bank Account Number: IL150120690000003111111 . Can you please check the status on bank account 954567876544?\n",
      "\n",
      "Kate's social security number is 078-05-1126.  Her driver license? it is 1234567A.\n"
     ]
    }
   ],
   "source": [
    "with open(\"demo_text.txt\") as f:\n",
    "    demo_text = f.readlines()\n",
    "    demo_text = \"\".join(demo_text)\n",
    "    \n",
    "print(demo_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bdc9375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_length::: 609\n",
      "splitting the text into chunks, length:: 609 > 512\n",
      "chunk_indexes::: [[0, 600], [560, 609]]\n",
      "\n",
      "\n",
      "prediction::: [{'entity_group': 'PATIENT', 'score': 0.99971944, 'word': ' David', 'start': 74, 'end': 79}, {'entity_group': 'PATIENT', 'score': 0.9997254, 'word': ' Johnson', 'start': 80, 'end': 87}, {'entity_group': 'LOC', 'score': 0.99989116, 'word': ' Maine', 'start': 102, 'end': 107}, {'entity_group': 'ID', 'score': 0.9778367, 'word': ' 40', 'start': 134, 'end': 136}, {'entity_group': 'PHONE', 'score': 0.9174068, 'word': '95', 'start': 136, 'end': 138}, {'entity_group': 'PHONE', 'score': 0.9045687, 'word': '-2609-9393-4932', 'start': 138, 'end': 153}, {'entity_group': 'ID', 'score': 0.70982337, 'word': ' 16Yeky6GM', 'start': 181, 'end': 190}, {'entity_group': 'ID', 'score': 0.43062696, 'word': 'N', 'start': 196, 'end': 197}, {'entity_group': 'ID', 'score': 0.5441897, 'word': 'BY7', 'start': 198, 'end': 201}, {'entity_group': 'DATE', 'score': 0.998892, 'word': ' September', 'start': 221, 'end': 230}, {'entity_group': 'DATE', 'score': 0.9992986, 'word': ' 18', 'start': 231, 'end': 233}, {'entity_group': 'PATORG', 'score': 0.48782215, 'word': ' micro', 'start': 244, 'end': 249}, {'entity_group': 'PHONE', 'score': 0.9918119, 'word': ' 192', 'start': 312, 'end': 315}, {'entity_group': 'PHONE', 'score': 0.7416994, 'word': '168', 'start': 316, 'end': 319}, {'entity_group': 'PHONE', 'score': 0.7953071, 'word': '0', 'start': 320, 'end': 321}, {'entity_group': 'ID', 'score': 0.9527074, 'word': ' 191280', 'start': 339, 'end': 345}, {'entity_group': 'ID', 'score': 0.64261794, 'word': '342', 'start': 345, 'end': 348}, {'entity_group': 'PHONE', 'score': 0.9995639, 'word': ' (212)', 'start': 370, 'end': 375}, {'entity_group': 'PHONE', 'score': 0.9946617, 'word': ' 555-1234', 'start': 376, 'end': 384}, {'entity_group': 'ID', 'score': 0.95471525, 'word': ' IL1501206900000031111', 'start': 438, 'end': 459}, {'entity_group': 'ID', 'score': 0.55189174, 'word': '11', 'start': 459, 'end': 461}, {'entity_group': 'ID', 'score': 0.84051114, 'word': ' 9545678765', 'start': 512, 'end': 522}, {'entity_group': 'ID', 'score': 0.58425415, 'word': '44', 'start': 522, 'end': 524}, {'entity_group': 'PATIENT', 'score': 0.8398881, 'word': 'Kate', 'start': 527, 'end': 531}, {'entity_group': 'PHONE', 'score': 0.99734545, 'word': ' 078', 'start': 560, 'end': 563}, {'entity_group': 'PHONE', 'score': 0.91025513, 'word': '-05-1126', 'start': 563, 'end': 571}, {'entity_group': 'PHONE', 'score': 0.9850341, 'word': ' 078', 'start': 560, 'end': 563}, {'entity_group': 'PHONE', 'score': 0.70171314, 'word': '-05-1126', 'start': 563, 'end': 571}, {'entity_group': 'ID', 'score': 0.9945222, 'word': ' 12345', 'start': 600, 'end': 605}]\n"
     ]
    }
   ],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry, EntityRecognizer\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "import copy\n",
    "\n",
    "# configuration = {\n",
    "#     \"nlp_engine_name\": \"transformers\",\n",
    "#     \"models\": [\n",
    "#              {\"lang_code\": \"en\", \"model_name\": {\"spacy\": \"en_core_web_lg\", \"transformers\": \"obi/deid_roberta_i2b2\"} }]\n",
    "# }\n",
    "\n",
    "configuration = {\n",
    "        \"nlp_engine_name\": \"spacy\",\n",
    "        \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n",
    "    }\n",
    "\n",
    "# Create NLP engine based on configuration\n",
    "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "nlp_engine_with_spanish = provider.create_engine()\n",
    "\n",
    "registry = RecognizerRegistry()\n",
    "registry.load_predefined_recognizers()\n",
    "registry.add_recognizer(transformers_recognizer)\n",
    "registry.remove_recognizer(\"SpacyRecognizer\")\n",
    "\n",
    "# Pass the created NLP engine and supported_languages to the AnalyzerEngine\n",
    "analyzer = AnalyzerEngine(\n",
    "    nlp_engine=nlp_engine_with_spanish,registry=registry, \n",
    "    supported_languages=[\"en\"]\n",
    ")\n",
    "\n",
    "\n",
    "def split_text_to_word_chunks(input_length: int, chunk_length: int, overlap_length: int):\n",
    "\n",
    "    if input_length < chunk_length:\n",
    "        return [[0, input_length]]\n",
    "    if chunk_length <= overlap_length:\n",
    "        print(\"overlap_length should be shorter than chunk_length, setting overlap_length to by half of chunk_length\")\n",
    "        overlap_length = chunk_length // 2\n",
    "    return [\n",
    "        [i, min([i + chunk_length, input_length])]\n",
    "        for i in range(\n",
    "            0, input_length - overlap_length, chunk_length - overlap_length\n",
    "        )]\n",
    "\n",
    "\n",
    "model_max_length = 512\n",
    "# calculate inputs based on the text\n",
    "text_length = len(demo_text)\n",
    "print(\"text_length:::\", text_length)\n",
    "\n",
    "# Split text into chunks\n",
    "if text_length <= model_max_length:\n",
    "    \n",
    "    predictions = pipeline(demo_text)\n",
    "    \n",
    "else:\n",
    "    print(\"splitting the text into chunks, length::\", text_length, \">\", model_max_length)\n",
    "    predictions = list()\n",
    "    chunk_length = 600\n",
    "    text_overlap_length = 40\n",
    "    \n",
    "    chunk_indexes = split_text_to_word_chunks(\n",
    "        text_length, chunk_length, text_overlap_length\n",
    "        )\n",
    "    \n",
    "    print(\"chunk_indexes:::\", chunk_indexes)\n",
    "    \n",
    "    # iterate over text chunks and run inference\n",
    "    for chunk_start, chunk_end in chunk_indexes:\n",
    "        chunk_text = demo_text[chunk_start:chunk_end]\n",
    "        chunk_preds = pipeline_(chunk_text)\n",
    "        \n",
    "        # align indexes to match the original text - add to each position the value of chunk_start\n",
    "        aligned_predictions = list()\n",
    "        \n",
    "        \n",
    "        for prediction in chunk_preds:\n",
    "            prediction_tmp = copy.deepcopy(prediction)\n",
    "            prediction_tmp[\"start\"] += chunk_start\n",
    "            prediction_tmp[\"end\"] += chunk_start\n",
    "            aligned_predictions.append(prediction_tmp)\n",
    "\n",
    "        predictions.extend(aligned_predictions)\n",
    "\n",
    "\n",
    "print(\"\\n\\nprediction:::\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bea295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
